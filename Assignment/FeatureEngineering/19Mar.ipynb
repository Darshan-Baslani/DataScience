{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6bf4d2-a91c-49e0-96c5-377d5097b6f2",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60941676-aa9c-4266-a91b-3b8f748f3a6f",
   "metadata": {},
   "source": [
    "**Min-Max** scaling is a data preprocessing technique used to scale and transform features in a dataset to a specific range, usually between 0 and 1. \n",
    "\n",
    "**Example**: Suppose you have a dataset of exam scores with a range of 60 to 100. Applying Min-Max scaling would transform these scores into a range between 0 and 1. If a score is 80, the scaled value would be (80 - 60) / (100 - 60) = 0.5. This normalization process ensures that each score's influence on the model is consistent regardless of its original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c654279-7423-426c-8c2e-57e4dc5b4889",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f96574-4181-4e02-b556-f9fbb410be71",
   "metadata": {},
   "source": [
    "The **Unit Vector technique**, also known as normalization, is a feature scaling method that scales each data point in a feature vector to have a magnitude of 1. Unlike Min-Max scaling, which brings data within a specific range (typically 0 to 1), normalization does not restrict the range of values and instead focuses on the direction of the vectors.\n",
    "\n",
    "**Example**: Consider a dataset of user ratings for different aspects of a product, such as design, performance, and features. Each user's ratings are recorded as a vector (design_rating, performance_rating, features_rating). Applying the Unit Vector technique involves dividing each vector by its Euclidean norm to normalize the vectors' magnitudes to 1. For a vector (4, 3, 2), the normalized vector would be (4/5, 3/5, 2/5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b675027-3def-48ec-b0c8-99eedbd9e44a",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197c77a-9d6a-43df-b056-a07c6bbb1542",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original variance as possible. It achieves this by identifying the principal components, which are orthogonal directions in the original feature space along which the data varies the most.\n",
    "\n",
    "Example: Let's say you have a dataset with features representing different measurements of various flowers, such as petal length, petal width, sepal length, and sepal width. These features exist in a 4-dimensional space. By applying PCA, you can find the principal components that capture the most significant variations in the data. Let's assume that the first two principal components capture most of the variance. You can then project your data onto these two principal components to obtain a 2-dimensional repre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48795f-875d-4207-9b42-cd9bdc3c20b4",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece2db8-454c-4ee4-a83f-22395d8d51ce",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to reduce the dimensionality of the data while retaining the most important information. Feature extraction involves transforming the original features into a new set of features that capture the underlying patterns or variations in the data.\n",
    "\n",
    "Example: Consider a dataset of facial images where each pixel's intensity value is treated as a feature. This results in a very high-dimensional feature space. By applying PCA as a feature extraction technique, you can identify the most significant variations in facial expressions, poses, or lighting conditions. Let's say the first few principal components capture the variations in smile intensity and head tilt. By projecting the original image data onto these components, you effectively extract features that represent these variations. These new features can be used as a reduced representation of the original data for tasks like facial expression recognition or facial clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6456d3-f2b6-49ff-8ee4-21eda7486310",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5279f10-2896-4e65-ae95-8626c413aa44",
   "metadata": {},
   "source": [
    "First, I will trasform all values of price, rating and delivery time in the range of 0 to 1, using min-max method. <br>\n",
    "This will help balance the dataset.<br>\n",
    "As, feature like price have higher value compared to rating and delivery time. This could lead to price having more impact in prediction. <br>\n",
    "Using min-max method all the values would have equal impact on recommandation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e22924-11dc-4cdf-983c-ca7974ccac6e",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe7de7-c32e-4919-8430-0f05c817bce4",
   "metadata": {},
   "source": [
    "In building a model to predict stock prices, many features are used. <br>\n",
    "PCA helps lessen this features. <br>\n",
    "With lessen features stock prices could be predicted faster. <br>\n",
    "Though, it could cause in data loss. <br>\n",
    "But, the prediction model would be much faster as PCA would have shorten the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c0174-c28d-47f8-99c5-90202452f014",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2427f8de-5fbf-4bf2-b696-31685387b664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "min_max = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "data = np.array([1,5,10,15,20]).reshape(-1,1)\n",
    "\n",
    "min_max.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90215f5-2dc3-4150-9b08-cbb28735d72c",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c76d3-0f0d-4397-89f1-863241c559c6",
   "metadata": {},
   "source": [
    "The number of principal components to retain in a PCA-based feature extraction process is a critical decision and depends on the specific characteristics of the data and the goals of the analysis. Generally, the number of principal components retained is determined based on the cumulative explained variance and the desired level of information retention.\n",
    "\n",
    "Here's how you could approach this:\n",
    "\n",
    "Standardize Data: Before applying PCA, it's a good practice to standardize the features to have zero mean and unit variance.\n",
    "\n",
    "Calculate PCA: Apply PCA to the standardized data. Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Explained Variance: Calculate the proportion of the total variance explained by each principal component. This is done by dividing each eigenvalue by the sum of all eigenvalues.\n",
    "\n",
    "Cumulative Explained Variance: Calculate the cumulative explained variance by adding up the explained variance values for each principal component in decreasing order.\n",
    "\n",
    "Decide on Retention: Decide on the number of principal components to retain based on the cumulative explained variance and your desired level of information retention. A common approach is to choose the number of components that collectively explain a high percentage of the total variance, such as 95% or 99%.\n",
    "\n",
    "Project Data: Use the selected principal components to project the data into a lower-dimensional space.\n",
    "\n",
    "The number of principal components to retain depends on your specific application. If you're using the extracted features for a classification or regression task, you might perform some experimentation to determine the optimal number of components that provides good model performance.\n",
    "\n",
    "For example, if you find that the first three principal components explain 90% of the total variance, you might choose to retain these three components. However, the choice can vary based on domain knowledge, the amount of data, and the specific requirements of your analysis.\n",
    "\n",
    "Ultimately, the decision of how many principal components to retain involves a trade-off between reducing dimensionality and retaining sufficient information for accurate modeling and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
