{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba07c03-8c0e-4608-9990-b2099b6e014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alex Net for MNIST in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2fa4b2-376b-4699-89dd-13a59bf41a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48cc3fa2-7ab0-4e0c-aea5-b7f0d734b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_out_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4), # (b, 96, 55, 55)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),   \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2) # (b, 96, 27, 27)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # (b, 256, 27, 27)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2) # (b, 256, 13, 13)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # (b, 384, 13, 13)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # (b, 384, 13, 13)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # (b, 256, 13, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2) # (b, 256, 6, 6)\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer8 = nn.Linear(4096, num_out_classes)\n",
    "\n",
    "    def init_bias(self):\n",
    "        # setting weights and biases of conv layers\n",
    "        # layer 1\n",
    "        nn.init.normal_(self.layer1[0].weight, mean=0, std=0.01)\n",
    "        nn.init.constant_(self.layer1[0].bias, 0)\n",
    "        # layer 2\n",
    "        nn.init.normal_(self.layer2[0].weight, mean=0, std=0.01)\n",
    "        nn.init.constant_(self.layer2[0].bias, 1)\n",
    "        # layer 3\n",
    "        nn.init.normal_(self.layer3[0].weight, mean=0, std=0.01)\n",
    "        nn.init.constant_(self.layer3[0].bias, 0)\n",
    "        # layer 4\n",
    "        nn.init.normal_(self.layer4[0].weight, mean=0, std=0.01)\n",
    "        nn.init.constant_(self.layer4[0].bias, 1)\n",
    "        # layer 5\n",
    "        nn.init.normal_(self.layer5[0].weight, mean=0, std=0.01)\n",
    "        nn.init.constant_(self.layer5[0].bias, 1)\n",
    "        # layer 6 - fully connected\n",
    "        nn.init.constant_(self.layer6[1].bias, 1)\n",
    "        # layer 7 - fully connected\n",
    "        nn.init.constant_(self.layer7[1].bias, 1)\n",
    "        # layer 8 - fully connected\n",
    "        nn.init.constant_(self.layer8.bias, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layer1(X)\n",
    "        print(X.shape)\n",
    "        X = self.layer2(X)\n",
    "        X = self.layer3(X)\n",
    "        X = self.layer4(X)\n",
    "        X = self.layer5(X)\n",
    "        X = X.view(-1, 256*6*6)\n",
    "        print(X.shape)\n",
    "        X = self.layer6(X)\n",
    "        X = self.layer7(X)\n",
    "        logits = self.layer8(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd7f6f-91f4-44f3-8527-a23618600de9",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- We would take input shape as 227 * 227, but the input shape in paper is 224*224, we would do this because 224*224 doesn't output the filter map in the shape 55*55 which is mentioned in the paper\n",
    "- We would take padding from layer 2 to layer 5 in conv. layer to match the output size given in paper, for layer 2 it is 27*27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "737c9dc9-ad76-4e43-b9c4-04a278cb2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer7): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer8): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(num_out_classes=1000)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "439d63b2-c8cf-42b1-a548-328cd6a30e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3876f65c-0a8a-4ff0-872c-4f7e077cb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(227),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ff4490-6dc0-4916-8a19-c6b132474720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [16:02<00:00, 177091.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "test_data = CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87524c84-cdbf-471d-8fcd-b1e45af9acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(shape=(32, 32, 3))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(plt.imshow(training_data.data[100]))\n",
    "training_data.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8438f969-6d7a-477e-81db-7ee4b57db38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([128, 3, 32, 32])\n",
      "Shape of y: torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# the data is converted into 256 batches\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beef456d-0cac-40c1-aa02-d74f1bbbc010",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87637af3-975f-4c13-b9eb-d3bb49cad422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for i, (X,y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if i%10 == 0:\n",
    "            print(f\"loss at {i} = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6bf2411-da72-4d30-8c8e-f48d0ccac031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 96, 2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x2x2). Calculated output size: (256x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 68\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(X)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 68\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(X)\n\u001b[1;32m     70\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(X)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (256x2x2). Calculated output size: (256x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4151feb-9792-4ad5-ab29-b7fa50a1372f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
